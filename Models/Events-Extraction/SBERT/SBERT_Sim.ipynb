{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re, sys, os\n","import numpy as np, pandas as pd\n","import json\n","import pickle as pkl\n","#import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","#from tqdm.notebook import tqdm_notebook"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import (accuracy_score, f1_score, classification_report)\n","from transformers import BertTokenizer, BertModel\n","from sentence_transformers import models\n","import torch, gc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from sentence_transformers import SentenceTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"running on device {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_query_candidate_cases():\n","    # get query and candidate case numbers\n","    query_nos = []\n","    candidate_nos = []\n","\n","    # fetch case numbers\n","    for item in os.listdir(test_dir_path + f'/query'):\n","        temp = int(item.strip('.txt'))\n","        query_nos.append(temp)\n","    for item in os.listdir(test_dir_path + f'/candidate'):\n","        temp = int(item.strip('.txt'))\n","        candidate_nos.append(temp)\n","    query_nos.sort()\n","    candidate_nos.sort()\n","    return query_nos, candidate_nos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_chunk_size = 64\n","def get_text_embedding(text, model):\n","    model.eval()\n","    with torch.no_grad():\n","        hidden_states = []\n","        for i in range(0,len(text), embedding_chunk_size):\n","            text_chunk = text[i : min(i+embedding_chunk_size, len(text))]\n","            hidden_state = model.encode(text_chunk)\n","            hidden_states.append(torch.from_numpy(hidden_state))\n","            del text_chunk, hidden_state  # cleaning ??, not the cause for memory overflows, check after fixing batch size issue \n","    ret = torch.vstack(hidden_states).squeeze(dim=1)\n","    del hidden_states\n","    return ret"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_embeddings_dict(dic:dict,model):\n","    keys = list(dic.keys())\n","    end_array, all_chunks = [], []\n","    __len__ = 0\n","    for i in dic.values():\n","        end_array.append(__len__)\n","        if len(i) == 0:\n","          all_chunks.extend([\" \"])\n","          __len__ += 1\n","        else:\n","          all_chunks.extend(i)\n","          __len__ += len(i)\n","    end_array.append(__len__)\n","    \n","    all_embeddings = []\n","    for chunk in tqdm(range(0, len(all_chunks), embedding_chunk_size), desc='Text chunks'):\n","        embedding_ = get_text_embedding(all_chunks[chunk : chunk + embedding_chunk_size], model)\n","        all_embeddings.extend(list(embedding_))\n","    \n","    output = {}\n","    # for i, key in enumerate(keys):\n","    #     output[key] = torch.stack(all_embeddings[end_array[i] : end_array[i+1]])\n","    for i, key in enumerate(keys):\n","        try : \n","          output[key] = torch.stack(all_embeddings[end_array[i] : end_array[i+1]])\n","        except:\n","            print(f\"error at iter {i}\\nerror for embeddings {all_embeddings[end_array[i] : end_array[i+1]]}\\n end_array[i] {end_array[i]}, end_array[i+1] {end_array[i+1]}, \")\n","            \n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_query_candidate_embeddings(model,segment_dictionary_data):\n","    # with open(test_dir_path +'/'+segment_dictionary_name, 'rb') as f:\n","    #     out = pkl.load(f, encoding=\"latin-1\")\n","    query_docs_segmented = segment_dictionary_data['dict_query']\n","    candidate_docs_segmented = segment_dictionary_data['dict_candidate']\n","    print(\"Starting for query_docs_segmented\")\n","    q = get_embeddings_dict(query_docs_segmented,model)\n","    print(\"Starting for candidate_docs_segmented\")\n","    c = get_embeddings_dict(candidate_docs_segmented,model)\n","    return q, c"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def relevance(args):\n","    query_num, candidate_num,SIMILARITY_MATRICES,query_embeddings,candidate_embeddings = args\n","\n","    #find pairwise similarity\n","    query_embed = query_embeddings[query_num]\n","    candidate_embed = candidate_embeddings[candidate_num]\n","    # print(\"For query:\",query_num,\" , Query_embed size:\",len(query_embed))\n","    # print(\"For candidate:\",candidate_num,\" , candidate_embed size:\",len(candidate_embed))\n","    q = query_embed.cuda()\n","    c = candidate_embed.cuda()\n","    q = q / q.norm(dim = 1)[:, None ]\n","    c = c / c.norm(dim = 1)[:, None ]\n","    similarity_matrix = torch.matmul(q, c.T)\n","    # SIMILARITY_MATRICES[int(query_num)][int(candidate_num)] = similarity_matrix\n","    return torch.max(similarity_matrix).item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_relevance(SIMILARITY_MATRICES,query_embeddings,candidate_embeddings):\n","    score_dict = {}\n","    for query_num in tqdm(query_embeddings.keys(), desc = 'Get similarity matrices'):\n","        SIMILARITY_MATRICES[int(query_num)] = {}\n","        temp_scores = []\n","        for candidate_num in candidate_embeddings.keys():\n","            __score__ = relevance((query_num, candidate_num,SIMILARITY_MATRICES,query_embeddings,candidate_embeddings))\n","            temp_scores.append(__score__)\n","            \n","        candidate_list = list(candidate_embeddings.keys())\n","        score_dict[query_num] = {candidate_list[count]:i for count, i in enumerate(temp_scores)}\n","    return score_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def similarity(model,i,model_id,segment_dictionary_name,segment_dictionary_data):\n","    model = model.to(device)\n","    print(\"Starting Similarity-------> \",i,\",\",segment_dictionary_name,\",\",model_id)\n","    SIMILARITY_MATRICES = {}\n","    query_embeddings, candidate_embeddings = get_query_candidate_embeddings(model,segment_dictionary_data)\n","    scores_dict = run_relevance(SIMILARITY_MATRICES,query_embeddings,candidate_embeddings)\n","    print(\"Similarity Calculation Finished------------>\",i,\",\",segment_dictionary_name,\",\",model_id)\n","    return scores_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('./SBERT_input_details.json','r') as f:\n","    input_details = json.load(f)\n","test_dir_path = input_details['data_dir_path'] #test_dir_path = r\"./indiankanoon/train\"\n","query_segment_dictionary_path = input_details['query_segment_dictionary_path']\n","candidate_segment_dictionary_path = input_details['candidate_segment_dictionary_path']\n","model_type = input_details['model_type']  #\"bert\"\n","model_id = input_details['model_id']   #\"0\"\n","dataset =input_details['dataset'] #\"ik\"\n","split_type = input_details['split_type']  #\"train\"\n","sim_csv_root = \"./Sim_CSVs/\"+dataset+\"/\"+model_type+\"_base\"+\"/\"\n","os.makedirs(sim_csv_root,exist_ok=True)\n","csv_file_name = dataset+\"_\"+split_type+\"_\"+model_type+\"_\"+str(model_id)+\".csv\"\n","if model_id == 0:\n","    # model_name = \"bert-base-uncased\" or \"distilroberta-base\"\n","    model_name = input_details['model_name']\n","    max_seq_length = 32\n","    # Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n","    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n","    # Apply mean pooling to get one fixed sized sentence vector\n","    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n","    model = SentenceTransformer(modules=[word_embedding_model, pooling_model]) # model = SentenceTransformer(model_path, device='cuda')\n","else:\n","    model_path = input_details['model_path']\n","    model = SentenceTransformer(model_path, device='cuda')\n","    \n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(query_segment_dictionary_path, 'rb') as f:\n","    query_segment_dictionary_data = pkl.load(f, encoding=\"latin-1\")\n","with open(candidate_segment_dictionary_path, 'rb') as f:\n","    candidate_segment_dictionary_data = pkl.load(f, encoding=\"latin-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query_data = query_segment_dictionary_data['query_data']\n","candidate_data = candidate_segment_dictionary_data['candidate_data']\n","segment_dictionary_data = {\"dict_query\":query_data,\"dict_candidate\":candidate_data}\n","segment_dictionary_name =  str(query_segment_dictionary_path.split(\"/\")[-1])+\"_and_\" + str(candidate_segment_dictionary_path.split(\"/\")[-1])\n","# \"segment_dictionary_\"+dataset+\"_\"+split_type"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of query docs: \",len(query_data))\n","print(\"Number of candidate docs: \",len(candidate_data))\n","scores_dict = similarity(model,0,model_id,segment_dictionary_name,segment_dictionary_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Saving Similarity CSV--------->\",segment_dictionary_name,\",\",model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_candidates = sorted(list(scores_dict[list(scores_dict.keys())[0]].keys()))\n","print(\"Length of all_candidates: \",len(all_candidates))\n","test_SBERT_sim = dict()\n","for query, sbert_score_dict in scores_dict.items():\n","    test_SBERT_sim[query] = list()\n","    for candidate in all_candidates:\n","        test_SBERT_sim[query].append(sbert_score_dict[candidate])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"sim_csv will be saved at \",sim_csv_root)\n","pd.DataFrame.from_dict(data=test_SBERT_sim, orient='index').to_csv(sim_csv_root+csv_file_name,header=all_candidates)\n","df = pd.read_csv(sim_csv_root+csv_file_name)\n","df.columns = ['query_case_id'] + list(df.columns[1:])\n","df.to_csv(sim_csv_root+csv_file_name,index=False)\n","print(\"Similarity CSV saved at: \",sim_csv_root+csv_file_name)\n","print(\"***************Finished Similarity for model: \",segment_dictionary_name,\",\",model_id,\" ******************\")\n","print(\"Finished\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
