{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import string\n","from tqdm import tqdm\n","import spacy\n","import os\n","import json\n","import re\n","import pickle\n","import time\n","# import concurrent.futures as cf\n","# from concurrent.futures import ProcessPoolExecutor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spacy.require_gpu()\n","global nlp\n","nlp = spacy.load(\"en_core_web_trf\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["alphabet_string = string.ascii_lowercase\n","alphabet_list = list(alphabet_string)\n","exclusion_list = alphabet_list + [\n","    \"no\",\n","    \"nos\",\n","    \"sub-s\",\n","    \"subs\",\n","    \"ss\",\n","    \"cl\",\n","    \"dr\",\n","    \"mr\",\n","    \"mrs\",\n","    \"dr\",\n","    \"vs\",\n","    \"ch\",\n","    \"addl\",\n","]\n","exclusion_list = [word + \".\" for word in exclusion_list]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess(content):\n","    raw_text = re.sub(r\"\\xa0\", \" \", content)\n","    raw_text = raw_text.split(\"\\n\")  # splitting using new line character\n","    text = raw_text.copy()\n","    text = [re.sub(r'[^a-zA-Z0-9.,<>)\\-(/?\\t ]', '', sentence)\n","            for sentence in text]\n","    # text1 = [re.sub(r'(?<=[^0-9])/(?=[^0-9])',' ',sentence) for sentence in text1]\n","    text = [re.sub(\"\\t+\", \" \", sentence) for sentence in text]\n","    # converting multiple tabs and spaces ito a single tab or space\n","    text = [re.sub(\"\\s+\", \" \", sentence) for sentence in text]\n","    text = [re.sub(\" +\", \" \", sentence) for sentence in text]\n","    # these were the commmon noises in out data, depends on data\n","    text = [re.sub(\"\\.\\.+\", \"\", sentence) for sentence in text]\n","    text = [re.sub(\"\\A ?\", \"\", sentence) for sentence in text]\n","    text = [sentence for sentence in text if(\n","        len(sentence) != 1 and not re.fullmatch(\"(\\d|\\d\\d|\\d\\d\\d)\", sentence))]\n","    text = [sentence for sentence in text if len(sentence) != 0]\n","    text = [re.sub('\\A\\(?(\\d|\\d\\d\\d|\\d\\d|[a-zA-Z])(\\.|\\))\\s?(?=[A-Z])', '\\n', sentence)\n","            for sentence in text]  # dividing into para wrt to points\n","    text = [re.sub(\"\\A\\(([ivx]+)\\)\\s?(?=[a-zA-Z0-9])\", '\\n', sentence)\n","            for sentence in text]  # dividing into para wrt to roman points\n","    text = [re.sub(r\"[()[\\]\\\"$']\", \" \", sentence) for sentence in text]\n","    text = [re.sub(r\" no.\", \" number \", sentence, flags=re.I)\n","            for sentence in text]\n","    text = [re.sub(r\" nos.\", \" numbers \", sentence, flags=re.I)\n","            for sentence in text]\n","    text = [re.sub(r\" co.\", \" company \", sentence) for sentence in text]\n","    text = [re.sub(r\" ltd.\", \" limited \", sentence, flags=re.I)\n","            for sentence in text]\n","    text = [re.sub(r\" pvt.\", \" private \", sentence, flags=re.I)\n","            for sentence in text]\n","    text = [re.sub(r\" vs\\.?\", \" versus \", sentence, flags=re.I)\n","            for sentence in text]\n","    text = [re.sub(r\"ors\\.?\", \"others\", sentence, flags=re.I)\n","            for sentence in text]\n","    # text = [re.sub(\"\\s+\",\" \",sentence) for sentence in text]\n","    text2 = []\n","    for index in range(len(text)):  # for removing multiple new-lines\n","        if(index > 0 and text[index] == '' and text[index-1] == ''):\n","            continue\n","        if(index < len(text)-1 and text[index+1] != '' and text[index+1][0] == '\\n' and text[index] == ''):\n","            continue\n","        text2.append(text[index])\n","    text = text2\n","    text = \"\\n\".join(text)\n","    lines = text.split(\"\\n\")\n","    text_new = \" \".join(lines)\n","    text_new = re.sub(\" +\", \" \", text_new)\n","    l_new = []\n","    for token in text_new.split():\n","        if token.lower() not in exclusion_list:\n","            l_new.append(token.strip())\n","    return \" \".join(l_new)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n","OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]\n","ADJECTIVES = [\n","    \"acomp\",\n","    \"advcl\",\n","    \"advmod\",\n","    \"amod\",\n","    \"appos\",\n","    \"nn\",\n","    \"nmod\",\n","    \"ccomp\",\n","    \"complm\",\n","    \"hmod\",\n","    \"infmod\",\n","    \"xcomp\",\n","    \"rcmod\",\n","    \"poss\",\n","    \" possessive\",\n","]\n","ADVERBS = [\"advmod\"]\n","COMPOUNDS = [\"compound\"]\n","PREPOSITIONS = [\"prep\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getSubsFromConjunctions(subs):\n","    moreSubs = []\n","    for sub in subs:\n","        # rights is a generator\n","        rights = list(sub.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if \"and\" in rightDeps:\n","            moreSubs.extend(\n","                [tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"]\n","            )\n","            if len(moreSubs) > 0:\n","                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n","    return moreSubs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getObjsFromConjunctions(objs):\n","    moreObjs = []\n","    for obj in objs:\n","        # rights is a generator\n","        rights = list(obj.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if \"and\" in rightDeps:\n","            moreObjs.extend(\n","                [tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"]\n","            )\n","            if len(moreObjs) > 0:\n","                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n","    return moreObjs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getVerbsFromConjunctions(verbs):\n","    moreVerbs = []\n","    for verb in verbs:\n","        rightDeps = {tok.lower_ for tok in verb.rights}\n","        if \"and\" in rightDeps:\n","            moreVerbs.extend(\n","                [tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n","            if len(moreVerbs) > 0:\n","                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n","    return moreVerbs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[56]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def findSubs(tok):\n","    head = tok.head\n","    # print(\"head.head: \",head.head)\n","    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n","        head = head.head\n","    if head.pos_ == \"VERB\":\n","        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n","        if len(subs) > 0:\n","            verbNegated = isNegated(head)\n","            subs.extend(getSubsFromConjunctions(subs))\n","            return subs, verbNegated\n","        elif head.head != head:\n","            return findSubs(head)\n","    elif head.pos_ == \"NOUN\":\n","        return [head], isNegated(tok)\n","    return [], False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[57]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def isNegated(tok):\n","    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n","    for dep in list(tok.lefts) + list(tok.rights):\n","        if dep.lower_ in negations:\n","            return True\n","    return False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[58]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def find_negation(tok):\n","    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n","    for dep in list(tok.lefts):\n","        if dep.lower_ in negations:\n","            verb = dep.lower_ + \" \" + tok.lemma_\n","            verb_id = [dep.i, tok.i]\n","            return verb, verb_id\n","    #     for dep in list(tok.rights):\n","    #         if dep.lower_ in negations:\n","    #             return tok.lemma_ +\" \"+ dep.lower_\n","    verb = tok.lemma_\n","    verb_id = [tok.i]\n","    return verb, verb_id"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[59]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def findSVs(tokens):\n","    svs = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n","    for v in verbs:\n","        subs, verbNegated = getAllSubs(v)\n","        if len(subs) > 0:\n","            for sub in subs:\n","                svs.append(\n","                    (sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n","    return svs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[60]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getObjsFromPrepositions(deps):\n","    objs = []\n","    # print(\"deps are: \", deps)\n","    for dep in deps:\n","        # print(\"For \",dep, \"pos: \",dep.pos_,\" and dep: \",dep.dep_)\n","        # if\n","        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or dep.dep_ == \"agent\"):\n","            # print(\"dep.rights are \",list(dep.rights))\n","            for tok in dep.rights:\n","                if (tok.pos_ == \"NOUN\" and tok.dep_ in OBJECTS) or (\n","                    tok.pos_ == \"PRON\" and tok.lower_ == \"me\"\n","                ):\n","                    objs.append(tok)\n","                elif tok.dep_ == \"pcomp\":\n","                    for t in tok.rights:\n","                        if (t.pos_ == \"NOUN\" and t.dep_ in OBJECTS) or (\n","                            t.pos_ == \"PRON\" and t.lower_ == \"me\"\n","                        ):\n","                            objs.append(t)\n","                else:\n","                    objs.extend(getObjsFromPrepositions(tok.rights))\n","            # objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n","    return objs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[61]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getAdjectives(toks):\n","    toks_with_adjectives = []\n","    for tok in toks:\n","        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n","        adjs.append(tok)\n","        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n","        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n","        toks_with_adjectives.extend(adjs)\n","    return toks_with_adjectives"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[62]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getObjsFromAttrs(deps):\n","    for dep in deps:\n","        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n","            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n","            if len(verbs) > 0:\n","                for v in verbs:\n","                    rights = list(v.rights)\n","                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","                    objs.extend(getObjsFromPrepositions(rights))\n","                    if len(objs) > 0:\n","                        return v, objs\n","    return None, None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[63]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getObjFromXComp(deps):\n","    for dep in deps:\n","        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n","            v = dep\n","            rights = list(v.rights)\n","            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","            objs.extend(getObjsFromPrepositions(rights))\n","            if len(objs) > 0:\n","                return v, objs\n","    return None, None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[64]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getAllSubs(v):\n","    verbNegated = isNegated(v)\n","    # print(\"For \", v ,\" v.lefts are \", list(v.lefts))\n","    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n","    # print(\"getAllSubs for \",v, subs )\n","    if len(subs) > 0:\n","        subs.extend(getSubsFromConjunctions(subs))\n","    else:\n","        foundSubs, verbNegated = findSubs(v)\n","        subs.extend(foundSubs)\n","    return subs, verbNegated"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[65]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getAllObjs(v):\n","    # rights is a generator\n","    rights = list(v.rights)\n","    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","    # print(\"For \",v,\" rights are \",rights,\" and objs are \",objs)\n","    objs.extend(getObjsFromPrepositions(rights))\n","    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n","    if (\n","        potentialNewVerb is not None\n","        and potentialNewObjs is not None\n","        and len(potentialNewObjs) > 0\n","    ):\n","        objs.extend(potentialNewObjs)\n","        v = potentialNewVerb\n","    if len(objs) > 0:\n","        objs.extend(getObjsFromConjunctions(objs))\n","    else:\n","        # print(\"No OBJECTS\")\n","        objs.extend(getObjsFromVerbConj(v))\n","    return v, objs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[66]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getAllObjsWithAdjectives(v):\n","    # rights is a generator\n","    rights = list(v.rights)\n","    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","    # print(\"For \",v,\" rights are \",rights,\" and objs are \",objs)\n","    if len(objs) == 0:\n","        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n","    objs.extend(getObjsFromPrepositions(rights))\n","    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n","    if (\n","        potentialNewVerb is not None\n","        and potentialNewObjs is not None\n","        and len(potentialNewObjs) > 0\n","    ):\n","        objs.extend(potentialNewObjs)\n","        v = potentialNewVerb\n","    if len(objs) > 0:\n","        objs.extend(getObjsFromConjunctions(objs))\n","    else:\n","        # print(\"No OBJECTS\")\n","        objs.extend(getObjsFromVerbConj(v))\n","    return v, objs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[67]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getObjsFromVerbConj(v):\n","    objs = []\n","    rights = list(v.rights)\n","    # print(\"v.rights :\", rights)\n","    for right in rights:\n","        if right.dep_ == \"conj\":\n","            subs, verbNegated = getAllSubs(right)\n","            objs.extend(subs)\n","        else:\n","            objs.extend(getObjsFromVerbConj(right))\n","    return objs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def findSVOs(tokens, len_doc):\n","    svos = []\n","    svo_token_ids = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n","    # print(\"Verbs: \",verbs,\" size is: \",len(verbs))\n","    for v in verbs:\n","        subs, verbNegated = getAllSubs(v)\n","        verb, verb_id = find_negation(v)\n","        # print(\"For \",v,\" subs are \",subs)\n","        # if no subs, don't examine this verb any longer\n","        if len(subs) > 0:\n","            v, objs = getAllObjs(v)\n","            # print(\"For \",v,\" objs are \",objs)\n","            for sub in subs:\n","                for obj in objs:\n","                    sub_compound = generate_compound(sub)\n","                    obj_compound = generate_compound(obj)\n","                    sub_flag, sub_tag = check_tag(sub_compound)\n","                    obj_flag, obj_tag = check_tag(obj_compound)\n","                    if obj_flag and sub_flag:\n","                        event = (sub_tag, verb, obj_tag)\n","                    elif obj_flag:\n","                        event = (\n","                            \" \".join(tok.lemma_ for tok in sub_compound),\n","                            verb,\n","                            obj_tag,\n","                        )\n","                    elif sub_flag:\n","                        event = (\n","                            sub_tag,\n","                            verb,\n","                            \" \".join(tok.lemma_ for tok in obj_compound),\n","                        )\n","                    else:\n","                        event = (\n","                            \" \".join(tok.lemma_ for tok in sub_compound),\n","                            verb,\n","                            \" \".join(tok.lemma_ for tok in obj_compound),\n","                        )\n","                    svos.append(event)\n","    return svos, svo_token_ids"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[23]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def findSVAOs(tokens):\n","    svos = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n","    # print(\"Verbs: \",verbs)\n","    for v in verbs:\n","        subs, verbNegated = getAllSubs(v)\n","        # if no subs, don't examine this verb any longer\n","        # print(\"For \",v,\" subs are \",subs)\n","        if len(subs) > 0:\n","            v, objs = getAllObjsWithAdjectives(v)\n","            # print(\"For \",v,\" objs are \",objs)\n","            for sub in subs:\n","                for obj in objs:\n","                    objNegated = isNegated(obj)\n","                    obj_desc_tokens = generate_left_right_adjectives(obj)\n","                    sub_compound = generate_compound(sub)\n","                    # verb_compound = generate_verb_advmod(v)\n","                    svos.append(\n","                        (\n","                            \" \".join(tok.lower_ for tok in sub_compound),\n","                            v.lower_,\n","                            \" \".join(tok.lower_ for tok in obj_desc_tokens),\n","                        )\n","                    )\n","    return svos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def check_tag(compound):\n","    flag = False\n","    res = \"\"\n","    for token in compound:\n","        if token.ent_type_ == \"PERSON\":\n","            flag = True\n","            res = \"<NAME>\"\n","            # print(token.text,\"----\",res)\n","            break\n","        elif token.ent_type_ == \"ORG\":\n","            flag = True\n","            res = \"<ORG>\"\n","            # print(token.text,\"----\",res)\n","            break\n","    return flag, res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_compound(token):\n","    token_compunds = []\n","    for tok in token.lefts:\n","        if tok.dep_ in COMPOUNDS:\n","            token_compunds.extend(generate_compound(tok))\n","    token_compunds.append(token)\n","    for tok in token.rights:\n","        if tok.dep_ in COMPOUNDS:\n","            token_compunds.extend(generate_compound(tok))\n","    return token_compunds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_verb_advmod(v):\n","    v_compunds = []\n","    for tok in v.lefts:\n","        if tok.dep_ in ADVERBS:\n","            v_compunds.extend(generate_verb_advmod(tok))\n","    v_compunds.append(v)\n","    for tok in v.rights:\n","        if tok.dep_ in ADVERBS:\n","            v_compunds.extend(generate_verb_advmod(tok))\n","    return v_compunds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[74]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_left_right_adjectives(obj):\n","    obj_desc_tokens = []\n","    for tok in obj.lefts:\n","        if tok.dep_ in ADJECTIVES:\n","            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n","    obj_desc_tokens.append(obj)\n","    for tok in obj.rights:\n","        if tok.dep_ in ADJECTIVES:\n","            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n","    return obj_desc_tokens"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[75]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["single_words = [\"a\", \"A\", \"<\", \">\", \"i\", \"I\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_special_characters(text):\n","    regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n","    text_returned = re.sub(regex, \" \", text)\n","    tokens = text_returned.split()\n","    words = []\n","    for word in tokens:\n","        if len(word) > 1 or word in single_words:\n","            # stemming and removing stopwords from the tokens\n","            words.append(word)\n","    out = \" \".join(words)\n","    # print(\"count of ! is : \", out.count(\"!\"))\n","    return \" \".join(words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["global_event_dict = dict()\n","global_event_line_dict = dict()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def insert_event_in_global_dict(file_id, i, line, SVO):\n","    for eve in SVO:\n","        # here eve is a tuple so converting into string\n","        eve = \" \".join(eve)\n","        if eve not in global_event_line_dict:\n","            global_event_line_dict[eve] = {file_id: {i: line}}\n","        elif file_id not in global_event_line_dict[eve]:\n","            global_event_line_dict[eve][file_id] = {i: line}\n","        else:\n","            global_event_line_dict[eve][file_id][i] = line"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def events_extraction(content, file_id):\n","    content = preprocess(content)\n","    # Define the pattern for sentence splitting\n","    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n","    # Split the text into sentences using the pattern\n","    content_sents = re.split(pattern, content)\n","    # content_sents = content.split(\".\")\n","    file_svo = []\n","    file_svo_text = []\n","    len_doc = 0\n","    lines = []\n","    for i, line in enumerate(content_sents):\n","        line = line.strip()\n","        lines.append(remove_special_characters(line))\n","    for i, doc in enumerate(nlp.pipe(lines)):\n","        SVO, SVO_Token_IDs = findSVOs(doc, len_doc)\n","        if len(SVO) > 0:\n","            file_svo.append(SVO)\n","            insert_event_in_global_dict(file_id, i, lines[i], SVO)\n","            for eve in SVO:\n","                file_svo_text.append(\" \".join(eve))\n","    return file_svo, file_svo_text, lines"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # load input_details.json\n","    with open('input_details.json') as f:\n","        input_details = json.load(f)\n","    # name of the dataset #train-test-dev #dir of files\n","    #\"ilpcr\", \"train\", \"candidate\"\n","    dataset, split_type, files_type = input_details[\n","        \"dataset\"], input_details[\"split_type\"], input_details[\"files_type\"]\n","    # path for data files\n","    input_root = input_details[\"input_root\"]\n","    roots = input_root+\"/\"+dataset+\"/\"+split_type+\"/\"+files_type+\"/\"\n","    # path to store files with events\n","    output_root = input_details[\"output_root\"]\n","    outpath = output_root+\"/\"+dataset+\"/\"+split_type+\"/\"+files_type+\"/\"\n","    os.makedirs(outpath, exist_ok=True)\n","    sent_data_path = output_root+\"/\"+dataset+\"/\"+split_type+\"/\" + \\\n","        \"sent_data_\"+split_type+\"_\"+dataset+\"_\"+files_type+\".sav\"\n","    segment_dictionary_path = output_root+\"/\"+dataset+\"/\"+split_type+\"/\" + \\\n","        \"segment_dictionary_\"+split_type+\"_\"+dataset+\"_\"+files_type+\".sav\"\n","    event_doc_line_path = output_root+\"/\"+dataset+\"/\"+split_type+\"/\" + \\\n","        \"event_doc_line_text_\"+split_type+\"_\"+dataset+\"_\"+files_type+\".pkl\"\n","    file_lst = os.listdir(roots)\n","    print(\"Number of files: \", len(file_lst))\n","    contents = []\n","    for file in tqdm(file_lst):\n","        # print(\"Started for file: \", file)\n","        file_name = file.split(\".txt\")[0]\n","        file_id = int(file_name)\n","        file_path = os.path.join(roots, file)\n","        try:\n","            f = open(file_path, \"r\", encoding=\"utf-8\")\n","            content = f.read()\n","            f.close()\n","        except Exception as e:\n","            print(\"Except 1 for file name: \" + file_path + str(e) + \"\\n\")\n","        contents.append(content)\n","    file_svo_lst = []\n","    file_sent_lst = []\n","    file_segment_dictionary = {\"dict_\"+files_type: {}}\n","    for i in tqdm(range(len(contents))):\n","        file_id = int(file_lst[i].split(\".txt\")[0])\n","        file = contents[i]\n","        file_svo, file_svo_text, lines = events_extraction(file, file_id)\n","        file_svo_lst.append(file_svo)\n","        file_segment_dictionary[\"dict_\"+files_type][file_id] = file_svo_text\n","        file_sent_lst.append(lines)\n","    file_sent_dict = {files_type+\"_data\": {}}\n","    for i, file in enumerate(file_lst):\n","        file_sent_dict[files_type +\n","                       \"_data\"][int(file.split(\".txt\")[0])] = file_sent_lst[i]\n","        json_obj = json.dumps(file_svo_lst[i], indent=4)\n","        with open(outpath + file.split(\".txt\")[0] + \".json\", \"w\", encoding=\"utf-8\") as outfile:\n","            outfile.write(json_obj)\n","    with open(sent_data_path, 'wb') as f:\n","        pickle.dump(file_sent_dict, f)\n","    with open(segment_dictionary_path, 'wb') as f:\n","        pickle.dump(file_segment_dictionary, f)\n","    with open(event_doc_line_path, 'wb') as f:\n","        pickle.dump(global_event_line_dict, f)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
