{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os, json, time\n",
    "import numpy as np, pandas as pd, pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import evaluate_at_K\n",
    "import spacy\n",
    "\n",
    "import torch, gc    # clean torch cuda cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make save folder \n",
    "current_time = '_'.join(time.ctime().split()) + '_' + str(os.getpid())    # process pid is appended to ensure that experiments spawned at the same time don't overwrite each other.\n",
    "save_folder = f'./exp_results/EXP_{current_time}'\n",
    "os.makedirs(save_folder)\n",
    "\n",
    "# get device\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"running on device {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_files/configs_ILPCR/bert/config_1.json', 'r') as f:   # choose your config directory\n",
    "    config_dict = json.load(f)\n",
    "with open(save_folder + f'/config_file.json', 'w') as f:\n",
    "    json.dump(config_dict, f, indent = 4)\n",
    "\n",
    "path_candidate_cases = config_dict['path_prior_cases']\n",
    "path_query_cases = config_dict['path_current_cases']\n",
    "true_labels_json = config_dict['true_labels_json']\n",
    "checkpoint = config_dict['checkpoint']\n",
    "top512 = config_dict['top512']\n",
    "dir_path = path_candidate_cases[:path_candidate_cases.rfind('/')]\n",
    "\n",
    "assert(os.path.isdir(path_candidate_cases))\n",
    "assert(os.path.isdir(path_query_cases))\n",
    "assert(os.path.isfile(true_labels_json))\n",
    "assert(top512 == \"True\" or top512 == \"False\")\n",
    "\n",
    "scores_save_path = f'{save_folder}/scores.json'     # dictionary containing the score between each query X candidate pair\n",
    "filled_sim_csv_path = f'{save_folder}/filled_similarity_matrix.csv' # csv containing the similarity matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Query and Candidate Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all query and candidate cases\n",
    "query_cases_nos = os.listdir(dir_path + '/query')\n",
    "query_docs = {}\n",
    "for i in query_cases_nos:\n",
    "    temp_path = dir_path + '/query/' + i\n",
    "    with open(temp_path, 'r', encoding = \"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    query_docs[int(re.findall(r'\\d+', i)[0])] = content\n",
    "\n",
    "candidate_cases_nos = os.listdir(dir_path + '/candidate')\n",
    "candidate_docs = {}\n",
    "for i in candidate_cases_nos:\n",
    "    temp_path = dir_path + '/candidate/' + i\n",
    "    with open(temp_path, 'r', encoding = 'utf-8') as f:\n",
    "        content = f.read()\n",
    "    candidate_docs[int(re.findall(r'\\d+', i)[0])] = content\n",
    "\n",
    "if 'coliee' in dir_path.lower():\n",
    "    padding_len = 6 \n",
    "elif 'ik' in dir_path.lower():\n",
    "    padding_len = 10\n",
    "else :\n",
    "    print(\"You appear to be using a new dataset, so set the padding_len explicitly. The padding length is the length of the numerical portion of the case id. The COLIEE dataset has a padding length of 6 and the ILPCR dataset has a padding length of 10.\")\n",
    "    raise RuntimeError\n",
    "\n",
    "def get_query_candidate_cases():\n",
    "    query_nos = []\n",
    "    candidate_nos = []\n",
    "\n",
    "    # fetch case numbers\n",
    "    for item in os.listdir(dir_path + f'/query'):\n",
    "        temp = int(item.strip('.txt'))\n",
    "        query_nos.append(temp)\n",
    "\n",
    "    for item in os.listdir(dir_path + f'/candidate'):\n",
    "        temp = int(item.strip('.txt'))\n",
    "        candidate_nos.append(temp)\n",
    "\n",
    "    query_nos.sort()\n",
    "    candidate_nos.sort()\n",
    "    return query_nos, candidate_nos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model checkpoint\n",
    "print(f\"Loading model {checkpoint}\")\n",
    "checkpoint_name = checkpoint[checkpoint.rfind('/')+1:]\n",
    "if 'distilbert' in checkpoint:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "elif 'bert' in checkpoint:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "elif 'longformer' in checkpoint:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "else :\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    if checkpoint == 'law-ai/InCaseLawBERT':\n",
    "        tokenizer.model_max_length = 510    # IncaseLawBERT checkpoint on hugging face doesn't have model_max_length setup.\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get case segments\n",
    "def get_base_case(k: int):\n",
    "    \"\"\"\n",
    "    Read a base case.\n",
    "    Example Usage : get_base_case(28)\n",
    "\n",
    "    Args:\n",
    "      k: Base case number\n",
    "    Returns:\n",
    "      One segmented base case.\n",
    "    \"\"\"\n",
    "    if(padding_len == 10):\n",
    "      path_to_file = dir_path + f\"/query/{k:010d}.txt\" # in query folder\n",
    "    elif(padding_len == 6):\n",
    "      path_to_file = dir_path + f\"/query/{k:06d}.txt\" # in query folder\n",
    "    \n",
    "    with open(path_to_file, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    contents = contents.replace('\\n',' ')\n",
    "    base_case = ' '.join(contents.split())\n",
    "    \n",
    "    return base_case\n",
    "\n",
    "def get_candidate_case( base_case_num: int, k: int):\n",
    "    if(padding_len == 10):\n",
    "      path_to_file = dir_path + f\"/candidate/{k:010d}.txt\"  # in candidate folder\n",
    "    elif(padding_len == 6):\n",
    "      path_to_file = dir_path + f\"/candidate/{k:06d}.txt\"  # in candidate folder\n",
    "  \n",
    "    with open(path_to_file, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    contents = contents.replace('\\n',' ')\n",
    "    candidate_case = ' '.join(contents.split())\n",
    "    \n",
    "    return candidate_case\n",
    "\n",
    "def example2seg(docs, max_sentences_per_segment : int = 1 , stride : int = 1, top_N_segments : int = int(1e6)):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/neuralmind-ai/coliee\n",
    "    Receives a document and segment it.\n",
    "    Example Usage : example2seg(get_candidate_case(query_nos[0], candidate_nos[0]), 4, 2, 3)\n",
    "\n",
    "    Args:\n",
    "      docs: Document\n",
    "      max_sentences_per_segment: number of sentences in each segment\n",
    "      stride: stride\n",
    "      top_N_segments : return only (top_N_segments) number of segments.\n",
    "    Returns:\n",
    "      One segmented document.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(docs)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    segments = []\n",
    "\n",
    "    for i in range(0, len(sentences), stride):\n",
    "      segment = ' '.join(sentences[i:i + max_sentences_per_segment])\n",
    "      segments.append(segment)\n",
    "      if i + max_sentences_per_segment >= len(sentences) or len(segments) >= top_N_segments :\n",
    "          break\n",
    "\n",
    "    return segments\n",
    "\n",
    "def save_top_N_segments(query_nos, candidate_nos, top_N = 25):\n",
    "    dict_query = {}\n",
    "    dict_candidate = {}\n",
    "    \n",
    "    for i in tqdm(range(len(query_nos)), desc = 'Saving query segments'):\n",
    "        base_case = query_nos[i]\n",
    "        base_doc = get_base_case(base_case)\n",
    "        base_segments = example2seg(base_doc, 10, 5, top_N_segments = top_N)\n",
    "        dict_query[base_case] = base_segments\n",
    "\n",
    "    for i in tqdm(range(len(candidate_nos)), desc = 'Saving candidate segments'):\n",
    "        candidate_case = candidate_nos[i]\n",
    "        candidate_doc = get_candidate_case(query_nos[0], candidate_case)\n",
    "        candidate_segments = example2seg(candidate_doc, 10, 5, top_N_segments = top_N)\n",
    "        dict_candidate[candidate_case] = candidate_segments\n",
    "    \n",
    "    ans = {'dict_query' : dict_query, 'dict_candidate' : dict_candidate}\n",
    "    return ans\n",
    "\n",
    "# make segments\n",
    "global nlp\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "query_nos, candidate_nos =  get_query_candidate_cases()\n",
    "_ = save_top_N_segments(query_nos, candidate_nos, np.inf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## get embeddings ##################\n",
    "embedding_chunk_size = 64\n",
    "def get_text_embedding(text, tokenizer, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = []\n",
    "        for i in range(0,len(text), embedding_chunk_size):\n",
    "            text_chunk = text[i : min(i+embedding_chunk_size, len(text))]\n",
    "            inputs = tokenizer(text_chunk, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            hidden_state = outputs.hidden_states[-1][:,0,:].to('cpu')\n",
    "            hidden_states.append(hidden_state)\n",
    "\n",
    "            del text_chunk, inputs, outputs, hidden_state\n",
    "\n",
    "    ret = torch.vstack(hidden_states).squeeze(dim=1)\n",
    "    del hidden_states\n",
    "    return ret\n",
    "\n",
    "def get_embeddings_dict(dic:dict):\n",
    "    '''\n",
    "    Computes the dense vector representation of the cases present in (dic) based on the globally set (top512) variable.\n",
    "    If (top512) is set to True, the document is passed as it is to the transformer model and longer documents are truncated.\n",
    "    If (top512) is set to False, the document is divided into segments and passed into the transformer model. The final representation is obtained by stacking the segment-wise vectors.\n",
    "    '''\n",
    "    keys = list(dic.keys())\n",
    "    output = {}\n",
    "    if top512 == \"False\":\n",
    "      end_array, all_chunks = [], []\n",
    "      __len__ = 0\n",
    "      for key, chunk in dic.items():\n",
    "        end_array.append(__len__)\n",
    "        \n",
    "        if len(chunk) == 0:\n",
    "          all_chunks.extend([\" \"])\n",
    "          __len__ += 1\n",
    "        else:\n",
    "          all_chunks.extend(chunk)\n",
    "          __len__ += len(chunk)\n",
    "      end_array.append(__len__)\n",
    "\n",
    "      for i in range(len(end_array)-1):\n",
    "        assert(end_array[i] != end_array[i+1])  # so that empty tensor doesn't go to torch.stack()\n",
    "      \n",
    "      all_embeddings = []\n",
    "      for chunk in tqdm(range(0, len(all_chunks), embedding_chunk_size), desc='Text chunks'):\n",
    "          embedding_ = get_text_embedding(all_chunks[chunk : chunk + embedding_chunk_size], tokenizer, model)\n",
    "          all_embeddings.extend(list(embedding_))\n",
    "      \n",
    "      output = {}\n",
    "      for i, key in enumerate(keys):\n",
    "          output[key] = torch.stack(all_embeddings[end_array[i] : end_array[i+1]])\n",
    "\n",
    "    elif top512 == \"True\":\n",
    "      for q_number, text in tqdm(dic.items()):\n",
    "        hidden_state = get_text_embedding([text], tokenizer, model)\n",
    "        output[q_number] = hidden_state\n",
    "    \n",
    "    else :\n",
    "      print(f\"Not expected : top512 is {top512}\")\n",
    "      raise RuntimeError\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_query_candidate_embeddings():\n",
    "    if top512 == \"False\":\n",
    "        with open(dir_path + f'/segment_dictionary.sav', 'rb') as f:    # load the segmented dictionaries\n",
    "          out = pkl.load(f)\n",
    "        query_docs_segmented = out['dict_query']\n",
    "        candidate_docs_segmented = out['dict_candidate']\n",
    "      \n",
    "        # Obtain vector representations using the segment dictionary.\n",
    "        q = get_embeddings_dict(query_docs_segmented)\n",
    "        c = get_embeddings_dict(candidate_docs_segmented)\n",
    "\n",
    "        q = { i:v.numpy()  for i,v in q.items()}\n",
    "        c = { i:v.numpy()  for i,v in c.items()}\n",
    "\n",
    "    if top512 == \"True\":\n",
    "        # Obtain vector representations using the case text.\n",
    "        q = get_embeddings_dict(query_docs)\n",
    "        c = get_embeddings_dict(candidate_docs)\n",
    "        q = { i:v.numpy()  for i,v in q.items()}\n",
    "        c = { i:v.numpy()  for i,v in c.items()}\n",
    "\n",
    "    return q, c\n",
    "  \n",
    "query_embeddings, candidate_embeddings = get_query_candidate_embeddings()\n",
    "del model, tokenizer  ### clear up the GPU\n",
    "\n",
    "################## compute relevance ##################\n",
    "def relevance(args):\n",
    "    query_num, candidate_num = args\n",
    "\n",
    "    #find pairwise similarity\n",
    "    with torch.no_grad():\n",
    "        query_embed = query_embeddings[query_num]\n",
    "        candidate_embed = candidate_embeddings[candidate_num]\n",
    "\n",
    "        q = torch.Tensor(query_embed).to(device)    # shift matrix multiplication to the GPU\n",
    "        c = torch.Tensor(candidate_embed).to(device)\n",
    "\n",
    "        q = q / q.norm(dim = 1)[:, None]\n",
    "        c = c / c.norm(dim = 1)[:, None]\n",
    "\n",
    "        similarity_matrix = torch.matmul(q, c.T)\n",
    "        ret = torch.max(similarity_matrix).cpu().item()\n",
    "        del q, c, similarity_matrix\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def run_relevance():\n",
    "    score_dict = {}\n",
    "    for query_num in tqdm(query_embeddings.keys(), desc = 'Getting similarity matrices'):\n",
    "        temp_scores = []\n",
    "        for candidate_num in candidate_embeddings.keys():\n",
    "            __score__ = relevance((query_num, candidate_num))\n",
    "            temp_scores.append(__score__)\n",
    "        candidate_list = list(candidate_embeddings.keys())\n",
    "        score_dict[query_num] = {candidate_list[count]:i for count, i in enumerate(temp_scores)}\n",
    "    \n",
    "    return score_dict\n",
    "\n",
    "score_dict = run_relevance()\n",
    "with open(scores_save_path, 'w') as f:  # save scores\n",
    "  json.dump(score_dict, f, indent = 4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_sim_df_from_labels(labels):\n",
    "    query_numbers = [int(re.findall(r'\\d+', i[\"id\"])[0]) for i in labels[\"Query Set\"]]\n",
    "    relevant_cases = [i[\"relevant candidates\"] for i in labels[\"Query Set\"]]\n",
    "    relevant_cases = [[int(re.findall(r'\\d+', j)[0]) for j in i] for i in relevant_cases] \n",
    "    relevant_cases = {i:j for i,j in zip(query_numbers, relevant_cases)}\n",
    "\n",
    "    candidate_numbers = [int(re.findall(r'\\d+', i[\"id\"])[0]) for i in labels[\"Candidate Set\"]]\n",
    "    candidate_numbers.sort()\n",
    "\n",
    "    row_wise_dataframe = {}\n",
    "    for query_number in sorted(list(relevant_cases.keys())):\n",
    "        relevance_dict = {} # contains 0 for not relevant, 1 for relevant, -1 for self-relevance/citation\n",
    "        for candidate in candidate_numbers:\n",
    "            if candidate == query_number:\n",
    "                relevance_dict[candidate] = -1\n",
    "            elif candidate in relevant_cases[query_number]:\n",
    "                relevance_dict[candidate] = 1\n",
    "            else :\n",
    "                relevance_dict[candidate] = 0\n",
    "\n",
    "        row_wise_dataframe[query_number] = relevance_dict\n",
    "\n",
    "    df = pd.DataFrame(row_wise_dataframe)\n",
    "    df = df.T\n",
    "    df.insert(loc=0, column='query_case_id', value=row_wise_dataframe.keys())\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "with open(true_labels_json, 'r') as f:\n",
    "    true_labels = json.load(f)  # get the gold labels file\n",
    "\n",
    "gold_labels_df = obtain_sim_df_from_labels(true_labels)\n",
    "sim_df = obtain_sim_df_from_labels(true_labels) # use the gold label file as a backbone to fill the similarity values.\n",
    "column_name = 'query_case_id' if 'query_case_id' in sim_df.columns else 'Unnamed: 0'\n",
    "for i, query in tqdm(enumerate(list(sim_df[column_name].values))):\n",
    "    assert(sim_df.iloc[i][column_name] == query)\n",
    "    sim_df.iloc[i] = [float(query)] +list(score_dict[query].values())\n",
    "\n",
    "sim_df.to_csv(filled_sim_csv_path)\n",
    "\n",
    "################ fix this ################\n",
    "output_numbers = evaluate_at_K.get_f1_vs_K(gold_labels_df, sim_df) # compute evaluation metrics\n",
    "with open(f'{save_folder}/output.json', 'w') as f:\n",
    "    json.dump(output_numbers, f, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
