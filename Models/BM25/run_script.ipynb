{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, re, string, codecs, random, string, numpy as np, pandas as pd\n",
    "import evaluate_at_K\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save folder location\n",
    "current_time = '_'.join(time.ctime().split()) + '_' + str(os.getpid())\n",
    "save_folder = f'./exp_results/EXP_{current_time}'\n",
    "os.makedirs(save_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = './config_files/ik_test/config_1.json'   # choose your own config\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "with open(save_folder + f'/config_file.json', 'w') as f:\n",
    "    json.dump(config_dict, f, indent = 4)   # save the config file with which the experiments are run\n",
    "\n",
    "path_prior_cases = config_dict['path_prior_cases']\n",
    "path_current_cases = config_dict['path_current_cases']\n",
    "true_labels_json = config_dict['true_labels_json']\n",
    "n_gram = config_dict['n_gram']\n",
    "\n",
    "assert(os.path.isdir(path_prior_cases))\n",
    "assert(os.path.isdir(path_current_cases))\n",
    "assert(os.path.isfile(true_labels_json))\n",
    "\n",
    "bm25_results_save_dict_path = f'{save_folder}/bm25_results.sav'     # saves the dictionary containing each query X candidate score\n",
    "filled_sim_csv_path = f'{save_folder}/filled_similarity_matrix.csv' # saves the similarity matrix containing each query X candidate score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25(object):\n",
    "    def __init__(self, b=0.7, k1=1.6, n_gram:int = 1):\n",
    "        self.n_gram = n_gram\n",
    "        self.vectorizer = TfidfVectorizer(max_df=.65, min_df=1,\n",
    "                                  use_idf=True, \n",
    "                                  ngram_range=(n_gram, n_gram))\n",
    "        \n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        print(f\"Fitting tf_idf vectorizer\")\n",
    "        self.vectorizer.fit(X)\n",
    "        print(f\"Finished tf_idf vectorizer, time : {time.perf_counter() - start_time:0.3f} sec\")\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_suffixes = (\".txt\")\n",
    "citation_file_paths = []\n",
    "for r, d, f in os.walk(path_prior_cases):   # candidate cases \n",
    "    for file in f:\n",
    "        if file.endswith(my_suffixes):\n",
    "            citation_file_paths.append(os.path.join(r, file))\n",
    "\n",
    "name_dict = {}\n",
    "corpus =[]\n",
    "citation_names = []\n",
    "for file in sorted(citation_file_paths):\n",
    "    f = codecs.open(file, \"r\", \"utf-8\", errors='ignore')\n",
    "    text = f.read()\n",
    "    corpus.append(text)\n",
    "    citation_names.append(os.path.basename(file))\n",
    "    name_dict[text] = os.path.basename(file)\n",
    "\n",
    "my_suffixes = (\".txt\")\n",
    "query_file_paths = []\n",
    "for r, d, f in os.walk(path_current_cases): # query cases\n",
    "    for file in f:\n",
    "        if file.endswith(my_suffixes):\n",
    "            query_file_paths.append(os.path.join(r, file))\n",
    "\n",
    "query_corpus = []\n",
    "query_names = [] \n",
    "\n",
    "#iterate throught the query database list in sorted manner\n",
    "# for file in tqdm_notebook(sorted(query_file_paths),desc = \"query documents\"):\n",
    "for file in sorted(query_file_paths):\n",
    "    open_file = open(file, 'r', encoding=\"utf-8\")\n",
    "    text = open_file.read()\n",
    "    \n",
    "    raw_str_list = text.splitlines()\n",
    "    str_list_3 = raw_str_list\n",
    "    query_corpus.append(''.join(str_list_3))\n",
    "    query_names.append(os.path.basename(file).zfill(14))\n",
    "    open_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain citations/labels for relevant cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(true_labels_json, 'r') as f:\n",
    "    true_labels = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run BM25 : Obtain TF-IDF vectorization of each candidate case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tf_idf vectorizer\n",
      "Finished tf_idf vectorizer, time : 3.993 sec\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25(n_gram = n_gram)\n",
    "bm25.fit(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run BM25 : compute BM25 scores for each (query, candidate) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_results_dict = {}\n",
    "\n",
    "# compute BM25 score between each query X candidate document\n",
    "for i in tqdm_notebook(range(len(query_corpus))):\n",
    "    with open(save_folder + f'/progress.txt', 'a+') as logger:\n",
    "        logger.write(f'Doing {i}/{len(query_corpus)}\\n')\n",
    "\n",
    "    qu = query_corpus[i]\n",
    "    qu_n = query_names[i]\n",
    "    \n",
    "    doc_scores = bm25.transform(qu, corpus)\n",
    "\n",
    "    assert(int(re.findall(r'\\d+',qu_n)[0]) not in bm_25_results_dict)\n",
    "\n",
    "    bm_25_results_dict[int(re.findall(r'\\d+',qu_n)[0])] = {int(re.findall(r'\\d+',citation_names[i])[0]) : doc_scores[i] for i in range(len(doc_scores))}\n",
    "\n",
    "with open(bm25_results_save_dict_path, 'wb') as f:  # save results\n",
    "    pkl.dump(bm_25_results_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the similarity matrix and compute the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_sim_df_from_labels(labels):\n",
    "    '''\n",
    "    Creates a Pandas DataFrame object from the ground truth labels.json file. \n",
    "    Useful for computing evaluation metrics.\n",
    "    '''\n",
    "    query_numbers = [int(re.findall(r'\\d+', i[\"id\"])[0]) for i in labels[\"Query Set\"]]\n",
    "    relevant_cases = [i[\"relevant candidates\"] for i in labels[\"Query Set\"]]\n",
    "    relevant_cases = [[int(re.findall(r'\\d+', j)[0]) for j in i] for i in relevant_cases] \n",
    "    relevant_cases = {i:j for i,j in zip(query_numbers, relevant_cases)}\n",
    "\n",
    "    candidate_numbers = [int(re.findall(r'\\d+', i[\"id\"])[0]) for i in labels[\"Candidate Set\"]]\n",
    "    candidate_numbers.sort()\n",
    "\n",
    "    row_wise_dataframe = {}\n",
    "    for query_number in sorted(list(relevant_cases.keys())):\n",
    "        relevance_dict = {} # contains 0 for not relevant, 1 for relevant, -1 for self-relevance/citation\n",
    "        for candidate in candidate_numbers:\n",
    "            if candidate == query_number:\n",
    "                relevance_dict[candidate] = -1\n",
    "            elif candidate in relevant_cases[query_number]:\n",
    "                relevance_dict[candidate] = 1\n",
    "            else :\n",
    "                relevance_dict[candidate] = 0\n",
    "\n",
    "        row_wise_dataframe[query_number] = relevance_dict\n",
    "\n",
    "    df = pd.DataFrame(row_wise_dataframe)\n",
    "    df = df.T\n",
    "    df.insert(loc=0, column='query_case_id', value=row_wise_dataframe.keys())\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "gold_labels_df = obtain_sim_df_from_labels(true_labels)\n",
    "sim_df = obtain_sim_df_from_labels(true_labels) # use the gold label as a backbone to fill the similarity values.\n",
    "sim_df.columns\n",
    "del_columns = [i for i in sim_df.columns if str(i).startswith('Unnamed')]    # pseudo column\n",
    "sim_df = sim_df.drop(columns=del_columns, axis = 1)\n",
    "column_candidates = list(sim_df.columns)[1:]\n",
    "column_name = 'query_case_id' if 'query_case_id' in sim_df.columns else 'Unnamed: 0'\n",
    "for i, query in tqdm_notebook(enumerate(list(sim_df[column_name].values))):\n",
    "    assert(sim_df.iloc[i][column_name] == query)\n",
    "    temp_bm25_scores = [bm_25_results_dict[query][int(i)] for i in column_candidates]\n",
    "    sim_df.iloc[i] = [float(query)] + temp_bm25_scores\n",
    "\n",
    "sim_df.to_csv(filled_sim_csv_path)\n",
    "\n",
    "# compute evaluation metrics\n",
    "output_numbers = evaluate_at_K.get_f1_vs_K(gold_labels_df, sim_df)\n",
    "with open(f'{save_folder}/output.json', 'w') as f:  # save results\n",
    "    json.dump(output_numbers, f, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
